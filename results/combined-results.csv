Model,Method,CaseHOLD,QNLI,ARC Easy,ARC Challenge,SciQ,MNLI,HellaSwag,Yelp,PIQA,MathQA,SQuAD
microsoft/deberta-v3-xsmall,Full Finetuning,0.8335897435897436,0.9112209408749771,0.2719298245614035,0.2785234899328859,0.718,0.8495160468670403,0.4552877912766381,0.9739,0.7176278563656148,0.2475977653631285
microsoft/deberta-v3-xsmall,LoRA 1,0.5184615384615384,0.8888888888888888,0.2754385964912281,0.26174496644295303,0.64,0.8183392766174223,0.35301732722565227,0.9675,0.6605005440696409,0.22033519553072625
microsoft/deberta-v3-xsmall,LoRA 2,0.5930769230769231,0.8929159802306426,0.26666666666666666,0.28187919463087246,0.555,0.828222109016811,0.3708424616610237,0.9682,0.6577801958650707,0.2241340782122905
microsoft/deberta-v3-xsmall,LoRA 4,0.6584615384615384,0.8943803770821893,0.28771929824561404,0.26174496644295303,0.622,0.8300560366785532,0.3877713602867955,0.9685,0.6605005440696409,0.22793296089385476
microsoft/deberta-v3-xsmall,LoRA 8,0.536923076923077,0.0,0.0,0.0,0.0,0.0,0.39623580959968135,0.9695,0.6626768226332971,0.22324022346368716
microsoft/deberta-v3-xsmall,LoRA 16,0.571025641025641,0.8976752699981695,0.26666666666666666,0.2785234899328859,0.592,0.8328069281711665,0.38179645488946423,0.9691,0.6523394994559304,0.2201117318435754
microsoft/deberta-v3-xsmall,LoRA 32,0.6415384615384615,0.9018854109463664,0.2771929824561403,0.2483221476510067,0.612,0.8315843097300051,0.4070902210714997,0.9693,0.6632208922742111,0.2201117318435754
microsoft/deberta-v3-xsmall,LoRA 64,0.6482051282051282,0.9004210140948197,0.2912280701754386,0.25838926174496646,0.605,0.8337238920020377,0.3974307906791476,0.9691,0.6610446137105549,0.2212290502793296
microsoft/deberta-v3-xsmall,LoRA 769,0.5823076923076923,0.900604063701263,0.2807017543859649,0.2550335570469799,0.547,0.832603158430973,0.39832702648874724,0.9695,0.6719260065288357,0.2223463687150838
microsoft/deberta-v3-xsmall,LoRA 1538,0.5417948717948718,0.9007871133077063,0.28596491228070176,0.25838926174496646,0.663,0.8337238920020377,0.39543915554670384,0.9692,0.6686615886833515,0.2212290502793296
microsoft/deberta-v3-xsmall,LoRA 2306,0.8771794871794871,0.914332784184514,0.26666666666666666,0.28859060402684567,0.259,0.8579724910850739,0.3791077474606652,0.9755,0.5048966267682263,0.22256983240223463
microsoft/deberta-v3-xsmall,LoRA 3075,0.8438461538461538,0.917993776313381,0.26666666666666666,0.2785234899328859,0.259,0.8563423331635253,0.33897629954192393,0.9749,0.514689880304679,0.23754189944134077
microsoft/deberta-v3-xsmall,LoRA 3843,0.4728205128205128,0.9015193117334798,0.2771929824561403,0.24496644295302014,0.559,0.8317880794701987,0.3964349731129257,0.9692,0.6496191512513602,0.2201117318435754
microsoft/deberta-v3-large,Full Finetuning,0.912051282051282,0.9586307889438038,0.26666666666666666,0.31208053691275167,0.87,0.9124808965868568,0.9220274845648277,0.9854,0.8688792165397171,0.22033519553072625
microsoft/deberta-v3-large,LoRA 1,0.8648717948717949,0.9514918542925133,0.2807017543859649,0.27181208053691275,0.889,0.9067753438614365,0.925413264289982,0.9838,0.8710554951033732,0.22793296089385476
microsoft/deberta-v3-large,LoRA 2,0.8694871794871795,0.9555189456342669,0.2771929824561403,0.28523489932885904,0.883,0.9063678043810495,0.9277036446922924,0.9843,0.8786724700761698,0.2259217877094972
microsoft/deberta-v3-large,LoRA 4,0.892051282051282,0.9542375983891634,0.28596491228070176,0.28859060402684567,0.884,0.90952623535405,0.9298944433379804,0.984,0.8835690968443961,0.2241340782122905
microsoft/deberta-v3-large,LoRA 8,0.9023076923076923,0.9547867472084935,0.26666666666666666,0.2986577181208054,0.889,0.9093224656138563,0.9344752041426011,0.9845,0.8868335146898803,0.21675977653631284
microsoft/deberta-v3-large,LoRA 16,0.8925641025641026,0.9540545487827201,0.2736842105263158,0.29194630872483224,0.879,0.9102394294447275,0.9340768771161123,0.9841,0.8841131664853101,0.22614525139664804
microsoft/deberta-v3-large,LoRA 32,0.8943589743589744,0.9569833424858136,0.3017543859649123,0.2684563758389262,0.885,0.9106469689251147,0.9331806413065127,0.9844,0.8862894450489662,0.2346368715083799
microsoft/deberta-v3-large,LoRA 64,0.8897435897435897,0.9551528464213802,0.2929824561403509,0.2953020134228188,0.891,0.9107488537952114,0.9331806413065127,0.9852,0.8911860718171926,0.23217877094972067
microsoft/deberta-v3-large,LoRA 886,0.8971794871794871,0.9557019952407102,0.28421052631578947,0.28859060402684567,0.891,0.9102394294447275,0.933379804819757,0.9841,0.8895538628944505,0.2288268156424581
microsoft/deberta-v3-large,LoRA 1771,0.8956410256410257,0.9540545487827201,0.28771929824561404,0.27181208053691275,0.885,0.9094243504839531,0.9348735311690898,0.985,0.8906420021762785,0.22636871508379888
microsoft/deberta-v3-large,LoRA 2656,0.8956410256410257,0.9557019952407102,0.2929824561403509,0.28859060402684567,0.876,0.913397860417728,0.9348735311690898,0.9848,0.8928182807399347,0.23128491620111732
microsoft/deberta-v3-large,LoRA 3541,0.8866666666666667,0.9557019952407102,0.28596491228070176,0.26174496644295303,0.885,0.9113601630157921,0.9330810595498905,0.9846,0.8933623503808488,0.23307262569832402
microsoft/deberta-v3-large,LoRA 4426,0.8943589743589744,0.9551528464213802,0.2789473684210526,0.2986577181208054,0.882,0.9103413143148242,0.9326827325234017,0.9843,0.8911860718171926,0.23217877094972067
microsoft/deberta-v2-xxlarge,Full Finetuning,0.8969230769230769,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 8,0.21051282051282053,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 1063,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 2126,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 3189,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 4252,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
microsoft/deberta-v2-xxlarge,LoRA 5314,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
gpt2,Full Finetuning,0.7938461538461539,0.886326194398682,0.2596491228070175,0.30201342281879195,0.633,0.78634742740703,0.25731925911173076,0.9705,0.5103373231773667,0.2341899441340782
gpt2,LoRA 1,0.7589743589743589,0.8599670510708401,0.2719298245614035,0.2785234899328859,0.259,0.7404992358634743,0.25731925911173076,0.9632,0.515778019586507,0.22100558659217878
gpt2,LoRA 2,0.7684615384615384,0.8683873329672341,0.2736842105263158,0.2684563758389262,0.259,0.7510952623535405,0.25980880302728543,0.9644,0.5092491838955386,0.22145251396648044
gpt2,LoRA 4,0.778974358974359,0.8735127219476478,0.26666666666666666,0.3187919463087248,0.26,0.7648497198166072,0.2575184226249751,0.965,0.5103373231773667,0.2198882681564246
gpt2,LoRA 8,0.7823076923076923,0.8828482518762585,0.2771929824561403,0.29194630872483224,0.262,0.7710646968925114,0.25731925911173076,0.965,0.5070729053318824,0.23307262569832402
gpt2,LoRA 16,0.798974358974359,0.8802855573860516,0.28771929824561404,0.30201342281879195,0.39,0.7741212429954152,0.25781716789484166,0.9659,0.515233949945593,0.22078212290502794
gpt2,LoRA 32,0.7828205128205128,0.8779059125022881,0.25087719298245614,0.28523489932885904,0.258,0.7708609271523179,0.25731925911173076,0.9659,0.514145810663765,0.22033519553072625
gpt2,LoRA 64,0.7902564102564102,0.8780889621087314,0.26842105263157895,0.3187919463087248,0.272,0.7722873153336729,0.25951005775741887,0.9649,0.514145810663765,0.22435754189944135
gpt2,LoRA 676,0.7879487179487179,0.8839465495149186,0.2719298245614035,0.3187919463087248,0.269,0.7758532857870606,0.25801633140808605,0.9667,0.5114254624591947,0.22145251396648044
gpt2,LoRA 1351,0.7879487179487179,0.8784550613216182,0.2736842105263158,0.32550335570469796,0.271,0.7759551706571575,0.25731925911173076,0.9654,0.514689880304679,0.23798882681564246
gpt2,LoRA 2026,0.7930769230769231,0.8802855573860516,0.2807017543859649,0.31543624161073824,0.261,0.7770759042282221,0.2577175861382195,0.9674,0.5081610446137106,0.2294972067039106
gpt2,LoRA 2701,0.7861538461538462,0.8790042101409482,0.27017543859649124,0.28187919463087246,0.259,0.7757514009169638,0.25731925911173076,0.9661,0.5125136017410229,0.2198882681564246
gpt2,LoRA 3376,0.7938461538461539,0.8764415156507414,0.28421052631578947,0.2986577181208054,0.259,0.7757514009169638,0.2574188408683529,0.9657,0.5070729053318824,0.2198882681564246
gpt2-large,Full Finetuning,0.8464102564102564,0.9311733479773019,0.256140350877193,0.28523489932885904,0.699,0.844319918492104,0.6187014538936467,0.9802,0.6735582154515778,0.26256983240223464
gpt2-large,LoRA 1,0.8035897435897436,0.919092073952041,0.27017543859649124,0.31208053691275167,0.666,0.8186449312277126,0.5733917546305517,0.9766,0.0,0.0
gpt2-large,LoRA 2,0.7861538461538462,0.9220208676551346,0.27017543859649124,0.3053691275167785,0.675,0.8201732042791645,0.5407289384584744,0.9764,0.0,0.0
gpt2-large,LoRA 4,0.7843589743589744,0.9220208676551346,0.2649122807017544,0.29194630872483224,0.672,0.8237391747325522,0.6032662816172077,0.9778,0.0,0.0
gpt2-large,LoRA 8,0.801025641025641,0.9207395204100312,0.24912280701754386,0.28523489932885904,0.676,0.8259806418746816,0.5700059749053973,0.978,0.6398258977149075,0.24491620111731843
gpt2-large,LoRA 16,0.8084615384615385,0.9222039172615779,0.2736842105263158,0.28523489932885904,0.695,0.8256749872643913,0.622585142401912,0.9778,0.0,0.0
gpt2-large,LoRA 32,0.8276923076923077,0.9247666117517848,0.2614035087719298,0.3053691275167785,0.682,0.8248599083036169,0.5135431189006174,0.9772,0.0,0.0
gpt2-large,LoRA 64,0.8143589743589743,0.923851363719568,0.2578947368421053,0.2986577181208054,0.688,0.8255731023942945,0.6260705038836885,0.9773,0.0,0.0
gpt2-large,LoRA 840,0.8194871794871795,0.9251327109646714,0.2596491228070175,0.31208053691275167,0.687,0.828222109016811,0.6338378809002191,0.978,0.0,0.0
gpt2-large,LoRA 1680,0.8002564102564103,0.9234852645066813,0.2631578947368421,0.30201342281879195,0.687,0.8268976057055527,0.6174068910575583,0.9777,0.0,0.0
gpt2-large,LoRA 2520,0.8120512820512821,0.9240344133260113,0.2631578947368421,0.30201342281879195,0.686,0.8299541518084564,0.58743278231428,0.978,0.0,0.0
gpt2-large,LoRA 3360,0.8017948717948717,0.9234852645066813,0.27017543859649124,0.3053691275167785,0.693,0.8252674477840041,0.6334395538737303,0.9776,0.0,0.0
gpt2-large,LoRA 4200,0.7923076923076923,0.9275123558484349,0.27017543859649124,0.30201342281879195,0.69,0.8258787570045848,0.6175064728141805,0.9781,0.0,0.0
llama2-7b,Full Finetuning,0.20615384615384616,0.0,0.0,0.0,0.272,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 1,0.20615384615384616,0.0,0.0,0.0,0.901,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 2,0.20615384615384616,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 4,0.20615384615384616,0.0,0.0,0.0,0.914,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 8,0.20615384615384616,0.0,0.0,0.0,0.913,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 16,0.20615384615384616,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 32,0.20615384615384616,0.0,0.0,0.0,0.917,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 64,0.0,0.0,0.0,0.0,0.912,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 2521,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 5042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 7562,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 10083,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
llama2-7b,LoRA 12603,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
google/t5-small-lm-adapt,Full Finetuning,0.8056410256410257,0.8733296723412045,0.2649122807017544,0.3221476510067114,0.595,0.7579215486500255,0.25731925911173076,0.9684,0.515233949945593,0.23150837988826817
google/t5-small-lm-adapt,LoRA 1,0.442051282051282,0.8261028738788212,0.2771929824561403,0.3187919463087248,0.396,0.6518593988792665,0.25761800438159727,0.9517,0.5206746463547334,0.22212290502793297
google/t5-small-lm-adapt,LoRA 2,0.4641025641025641,0.8367197510525353,0.2807017543859649,0.3087248322147651,0.399,0.6636780438104941,0.25761800438159727,0.9535,0.5190424374319913,0.22547486033519554
google/t5-small-lm-adapt,LoRA 4,0.7220512820512821,0.8407468423942889,0.2929824561403509,0.3187919463087248,0.272,0.7154355578196637,0.2551284604660426,0.9588,0.514145810663765,0.224804469273743
google/t5-small-lm-adapt,LoRA 8,0.757948717948718,0.8480688266520227,0.28771929824561404,0.3053691275167785,0.519,0.7210392256749872,0.2570205138418642,0.9599,0.5179542981501633,0.22681564245810057
google/t5-small-lm-adapt,LoRA 16,0.5115384615384615,0.8380010982976387,0.26666666666666666,0.31543624161073824,0.423,0.6581762608252675,0.25692093208524197,0.9554,0.515778019586507,0.22212290502793297
google/t5-small-lm-adapt,LoRA 32,0.7335897435897436,0.8497162731100129,0.2754385964912281,0.3053691275167785,0.554,0.7232806928171167,0.2575184226249751,0.9611,0.5059847660500544,0.22815642458100557
google/t5-small-lm-adapt,LoRA 64,0.7138461538461538,0.852462017206663,0.2736842105263158,0.2785234899328859,0.532,0.7276617422312787,0.25941047600079664,0.9628,0.5114254624591947,0.22636871508379888
google/t5-small-lm-adapt,LoRA 283,0.622051282051282,0.853011166025993,0.26842105263157895,0.30201342281879195,0.273,0.7339786041772797,0.2606054570802629,0.9611,0.5070729053318824,0.22636871508379888
google/t5-small-lm-adapt,LoRA 566,0.7569230769230769,0.8484349258649094,0.29473684210526313,0.3288590604026846,0.541,0.7285787060621498,0.2531368253335989,0.962,0.5076169749727966,0.2252513966480447
google/t5-small-lm-adapt,LoRA 849,0.48923076923076925,0.8394654951491854,0.2824561403508772,0.31208053691275167,0.404,0.6708099847172695,0.2552280422226648,0.955,0.5119695321001088,0.21966480446927375
google/t5-small-lm-adapt,LoRA 1132,0.6748717948717948,0.85722130697419,0.2789473684210526,0.31208053691275167,0.532,0.7230769230769231,0.25642302330213107,0.9618,0.5212187159956474,0.22167597765363128
google/t5-small-lm-adapt,LoRA 1414,0.7702564102564102,0.8579535053999634,0.26666666666666666,0.3288590604026846,0.527,0.729903209373408,0.25672176857199763,0.9621,0.514689880304679,0.22972067039106145
google/t5-large-lm-adapt,Full Finetuning,0.9038461538461539,0.9511257550796266,0.2649122807017544,0.28187919463087246,0.809,0.8976057055527255,0.8578968333001394,0.9844,0.5125136017410229,0.25094972067039106
google/t5-large-lm-adapt,LoRA 1,0.823076923076923,0.0,0.0,0.0,0.769,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 2,0.8294871794871795,0.0,0.0,0.0,0.789,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 4,0.8384615384615385,0.0,0.0,0.0,0.784,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 8,0.838974358974359,0.938129233022149,0.28771929824561404,0.30201342281879195,0.792,0.8850738665308202,0.824636526588329,0.9831,0.7970620239390642,0.22033519553072625
google/t5-large-lm-adapt,LoRA 16,0.8348717948717949,0.0,0.0,0.0,0.772,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 32,0.8353846153846154,0.0,0.0,0.0,0.781,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 64,0.8351282051282052,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 510,0.841025641025641,0.0,0.0,0.0,0.784,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 1020,0.8456410256410256,0.0,0.0,0.0,0.773,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 1529,0.8430769230769231,0.0,0.0,0.0,0.789,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 2039,0.8461538461538461,0.0,0.0,0.0,0.775,0.0,0.0,0.0,0.0,0.0
google/t5-large-lm-adapt,LoRA 2548,0.8464102564102564,0.0,0.0,0.0,0.774,0.0,0.0,0.0,0.0,0.0
t5-xxl,Full Finetuning,0.0,0.0,0.0,0.0,0.263,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
t5-xxl,LoRA 8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-small-lora-experiments,Full Finetuning,0.7887179487179488,0.8808347062053816,0.2719298245614035,0.28859060402684567,0.582,0.7614875191034132,0.26906990639314876,0.9666,0.5239390642002176,0.24536312849162012
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 1,0.31974358974358974,0.8420281896393923,0.3,0.2651006711409396,0.378,0.6609271523178808,0.25851424019119695,0.9507,0.5244831338411317,0.2198882681564246
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 2,0.29923076923076924,0.8376349990847519,0.2964912280701754,0.28859060402684567,0.374,0.6657157412124299,0.2559251145190201,0.9506,0.5239390642002176,0.22614525139664804
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 4,0.7084615384615385,0.8641771920190372,0.26666666666666666,0.31543624161073824,0.378,0.7209373408048905,0.25891256721768574,0.9586,0.514145810663765,0.2294972067039106
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 8,0.528974358974359,0.8683873329672341,0.26842105263157895,0.29194630872483224,0.519,0.72603158430973,0.26309500099581756,0.9592,0.5228509249183896,0.23083798882681564
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 16,0.2235897435897436,0.8497162731100129,0.29473684210526313,0.2751677852348993,0.379,0.6717269485481406,0.2548297151961761,0.9515,0.5201305767138193,0.22569832402234638
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 32,0.32769230769230767,0.8526450668131064,0.2824561403508772,0.2785234899328859,0.384,0.6723382577687214,0.25712009559848636,0.9533,0.5092491838955386,0.22502793296089385
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 64,0.7184615384615385,0.876258466044298,0.27017543859649124,0.3422818791946309,0.489,0.7320427916454406,0.2581159131647082,0.9603,0.5179542981501633,0.22860335195530726
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 283,0.7489743589743589,0.8707669778509977,0.2754385964912281,0.3221476510067114,0.525,0.7366276107997962,0.2592113124875523,0.9611,0.5228509249183896,0.2252513966480447
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 566,0.31,0.8497162731100129,0.28771929824561404,0.28187919463087246,0.373,0.6742740703005604,0.2599083847839076,0.953,0.5179542981501633,0.22502793296089385
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 848,0.7266666666666667,0.8727805235218744,0.2964912280701754,0.3053691275167785,0.542,0.7340804890473764,0.2566221868153754,0.9596,0.5108813928182807,0.2259217877094972
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 1131,0.5733333333333334,0.8758923668314114,0.27017543859649124,0.3053691275167785,0.527,0.7370351502801834,0.25791674965146383,0.9604,0.514689880304679,0.23106145251396648
jacobmorrison/tk-instruct-small-lora-experiments,LoRA 1413,0.5076923076923077,0.8731466227347611,0.27017543859649124,0.30201342281879195,0.507,0.7373408048904737,0.2605058753236407,0.9609,0.5184983677910773,0.22256983240223463
jacobmorrison/tk-instruct-large-lora-experiments,Full Finetuning,0.9041025641025641,0.9458173164927696,0.2649122807017544,0.31543624161073824,0.797,0.8882322975038207,0.8334993029277037,0.9831,0.5353645266594124,0.2594413407821229
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 1,0.8748717948717949,0.0,0.0,0.0,0.74,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 2,0.8828205128205128,0.0,0.0,0.0,0.757,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 4,0.8897435897435897,0.0,0.0,0.0,0.732,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 8,0.8982051282051282,0.9419732747574593,0.28771929824561404,0.28523489932885904,0.777,0.8808965868568518,0.6721768571997611,0.9804,0.7818280739934712,0.22346368715083798
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 16,0.898974358974359,0.0,0.0,0.0,0.773,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 32,0.8994871794871795,0.0,0.0,0.0,0.781,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 64,0.8976923076923077,0.0,0.0,0.0,0.782,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 510,0.9038461538461539,0.0,0.0,0.0,0.523,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 1020,0.9064102564102564,0.0,0.0,0.0,0.782,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 1529,0.8702564102564102,0.0,0.0,0.0,0.772,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 2039,0.9005128205128206,0.0,0.0,0.0,0.755,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-large-lora-experiments,LoRA 2548,0.9012820512820513,0.0,0.0,0.0,0.786,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,Full Finetuning,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
jacobmorrison/tk-instruct-xxl-lora-experiments,LoRA 8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
