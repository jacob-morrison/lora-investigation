# merge command
# dataset: 	01HC32D2FX1DF27Y8C4XB4G3Q0
MODEL_NAME=microsoft/deberta-v3-xsmall
gantry run \
        --beaker-image 'jacobm/lora-instruct' \
        --workspace ai2/interpolations \
        --cluster ai2/general-cirrascale \
        --cluster ai2/allennlp-cirrascale \
        --pip requirements.txt \
        --gpus 1 \
        --priority high \
        --dataset '01HC32D2FX1DF27Y8C4XB4G3Q0:/model/' \
    -- python -u experiments/merge_and_eval.py \
        --task_name case_hold \
        --model_name_or_path ${MODEL_NAME} \
        --do_eval \
        --do_predict \
        --use_lora False \
        --lora_rank 8 \
        --seed 1 \
        --per_device_eval_batch_size 8 \
        --max_seq_length 512 \
        --eval_accumulation_steps 1

        # --bf16 \ # use for llama
        # --fp16 \
        # --fp16_full_eval 

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' --workspace ai2/lexglue-tasks --cluster ai2/general-cirrascale \\
    --conda environment.yml --gpus 1 --priority high \\
    --dataset 'merge-t5-xl-sa-low:/t5-model' \\
    --dataset 'robustness-3-domains:/robustness-data' \\
    -- python -u main.py

# case hold
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='case_hold'

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/lexglue-tasks --cluster ai2/allennlp-cirrascale \
--pip requirements.txt --gpus 1 --priority high \
-- python -u experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir /results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--use_lora False --lora_rank 8 \
--load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 --learning_rate 3e-5 \
--per_device_train_batch_size 8 --per_device_eval_batch_size 8 --seed 1 --fp16 \
--fp16_full_eval --gradient_accumulation_steps 1 --eval_accumulation_steps 1





--------

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/lora-vs-full-ft --cluster ai2/allennlp-cirrascale --cluster ai2/allennlp-elanding-a100-40g \
--pip requirements.txt --gpus 4 --priority high \
-- accelerate launch experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir /results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--max_seq_length 1024 --use_lora False --lora_rank 8 --save_total_limit 5 \
--load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
steps --eval_steps 125 --save_strategy steps --save_steps 1250 --max_steps 18750 --learning_rate 5e-5 \
--per_device_train_batch_size 2 --per_device_eval_batch_size 8 --seed 1 --gradient_accumulation_steps 1 --dataloader_pin_memory False


local testing:

python -u experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir ../results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--max_seq_length 1024 --use_lora False --lora_rank 8 --save_total_limit 5 \
--load_best_model_at_end --metric_for_best_model accuracy --greater_is_better True --evaluation_strategy \
steps --eval_steps 100 --save_strategy steps --save_steps 10000 --max_steps 1000 --learning_rate 5e-4 \
--per_device_train_batch_size 8 --per_device_eval_batch_size 8 --seed 1 --gradient_accumulation_steps 1 --dataloader_pin_memory False --max_eval_samples 10 --max_train_samples 10


---

beaker testing:

accelerate launch \
    --mixed_precision bf16 \
    --num_machines 1 \
    --num_processes 8 \
    experiments/sequence_classification.py \
    --task_name case-hold --model_name_or_path /net/nfs.cirrascale/allennlp/yizhongw/hf_llama_models/7B \
    --output_dir /results/ --do_train --do_eval --do_predict --max_seq_length 1024 \
    --use_lora True --lora_rank 8 --save_total_limit 1 --load_best_model_at_end \
    --metric_for_best_model accuracy --greater_is_better True --evaluation_strategy steps \
    --eval_steps 1250 --save_strategy steps --save_steps 1250 \
    --max_steps 18750 --learning_rate 1e-4 --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 8 --seed 1 --gradient_accumulation_steps 1 \
    --use_flash_attn \
    --max_eval_samples 10 --max_train_samples 10 --max_predict_samples 10 \
    --dataloader_pin_memory False