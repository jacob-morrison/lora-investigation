gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' --workspace ai2/lexglue-tasks --cluster ai2/general-cirrascale \\
    --conda environment.yml --gpus 1 --priority high \\
    --dataset 'merge-t5-xl-sa-low:/t5-model' \\
    --dataset 'robustness-3-domains:/robustness-data' \\
    -- python -u main.py

# case hold
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='case_hold'

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/lexglue-tasks --cluster ai2/allennlp-cirrascale \
--pip requirements.txt --gpus 1 --priority high \
-- python -u experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir /results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--use_lora False --lora_rank 8 \
--load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 --learning_rate 3e-5 \
--per_device_train_batch_size 8 --per_device_eval_batch_size 8 --seed 1 --fp16 \
--fp16_full_eval --gradient_accumulation_steps 1 --eval_accumulation_steps 1





--------

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/lora-vs-full-ft --cluster ai2/allennlp-cirrascale \
--pip requirements.txt --gpus 4 --priority high \
-- accelerate launch experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir /results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--use_lora False --lora_rank 8 --save_total_limit 5 \
--load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
steps --eval_steps 125 --save_strategy steps --save_steps 1250 --max_steps 18750 --learning_rate 5e-5 \
--per_device_train_batch_size 2 --per_device_eval_batch_size 8 --seed 1 --gradient_accumulation_steps 1 --eval_accumulation_steps 1


local testing:

python -u experiments/case_hold.py --task_name case_hold --model_name_or_path ${MODEL_NAME} \
--output_dir ../results/logs/case_hold/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir \
--use_lora False --lora_rank 8 \
--load_best_model_at_end --metric_for_best_model exact_match --greater_is_better True --evaluation_strategy \
steps --eval_steps 100 --save_strategy steps --save_steps 10000 --max_steps 1000 --learning_rate 5e-4 \
--per_device_train_batch_size 8 --per_device_eval_batch_size 8 --seed 1 --gradient_accumulation_steps 1 --eval_accumulation_steps 1 --max_eval_samples 10 --max_train_samples 10